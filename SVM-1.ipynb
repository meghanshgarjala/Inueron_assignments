{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21564f01-9bb5-41d9-9f7a-038806b7425c",
   "metadata": {},
   "source": [
    "Q1. What is the mathematical formula for a linear SVM?\n",
    "\n",
    "\n",
    "Q2. What is the objective function of a linear SVM?\n",
    "\n",
    "\n",
    "Q3. What is the kernel trick in SVM?\n",
    "\n",
    "\n",
    "Q4. What is the role of support vectors in SVM Explain with example\n",
    "\n",
    "\n",
    "Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in\n",
    "SVM?\n",
    "\n",
    "\n",
    "Q6. SVM Implementation through Iris dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d736cef-ae50-4c46-a018-c704e6129620",
   "metadata": {},
   "source": [
    "Q1. What is the Mathematical Formula for a Linear SVM?\n",
    "For a binary classification problem, a linear Support Vector Machine (SVM) attempts to find the optimal hyperplane that best separates the two classes. The mathematical formula for a linear SVM is given by:\n",
    "𝑓(𝑥)=𝑤⊤𝑥+𝑏\n",
    "f(x)=w ⊤ x+b\n",
    "Where:\n",
    "𝑥 is the input feature vector.\n",
    "𝑤 is the weight vector that determines the orientation of the hyperplane.\n",
    "𝑏 is the bias term that shifts the hyperplane.\n",
    "The decision rule for classification is:\n",
    "If 𝑓(𝑥)≥0\n",
    "f(x)≥0, classify \n",
    "𝑥 as class +1.\n",
    "If 𝑓(𝑥)<0\n",
    "f(x)<0, classify \n",
    "𝑥 as class -1.\n",
    "\n",
    "\n",
    "4. \n",
    "Support vectors are the data points that are closest to the decision boundary (the hyperplane) and directly influence its position and orientation. They are crucial because the optimal hyperplane is determined by these support vectors.\n",
    "\n",
    "Example:\n",
    "\n",
    "Consider a 2D binary classification problem where the two classes are linearly separable. The SVM will find a hyperplane that maximizes the margin between the two classes. The points that lie on the edges of the margin (on the margin planes) are the support vectors.\n",
    "Even if all other points are removed, the decision boundary would remain the same as long as the support vectors are retained.\n",
    "\n",
    "\n",
    "5.\n",
    "To visualize these concepts:\n",
    "\n",
    "Hyperplane: The decision boundary that separates the two classes. In 2D, it's a line, and in 3D, it's a plane.\n",
    "Marginal Plane: The planes that are parallel to the hyperplane and pass through the support vectors, defining the margin.\n",
    "Soft Margin: Allows some misclassification (slack variables \n",
    "𝜉\n",
    "𝑖\n",
    "ξ \n",
    "i\n",
    "​\n",
    " ) to enable better generalization on noisy data.\n",
    "Hard Margin: Requires all data points to be correctly classified with no errors, which is only feasible if the data is perfectly linearly separable.\n",
    "Graphical Illustrations:\n",
    "\n",
    "Hyperplane and Marginal Planes: The hyperplane lies equidistant from the two marginal planes.\n",
    "Hard Margin: Both classes are perfectly separated by the hyperplane, with all points lying outside or on the marginal planes.\n",
    "Soft Margin: Some points may lie within the margin or even on the wrong side of the hyperplane, but the overall margin is maximized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac5b7a1-0d05-472b-b2bd-1cd30e4ae8f6",
   "metadata": {},
   "source": [
    "# Import necessary libraries\r\n",
    "from sklearn import datasets\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.svm import SVC\r\n",
    "from sklearn.metrics import classification_report, confusion_matrix\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "# Load the Iris dataset\r\n",
    "iris = datasets.load_iris()\r\n",
    "X = iris.data[:, :2]  # We take only the first two features for easy visualization\r\n",
    "y = iris.target\r\n",
    "\r\n",
    "# We will perform binary classification (only for classes 0 and 1)\r\n",
    "X = X[y != 2]\r\n",
    "y = y[y != 2]\r\n",
    "\r\n",
    "# Split the dataset into training and testing sets\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\r\n",
    "\r\n",
    "# Train a linear SVM classifier\r\n",
    "svm = SVC(kernel='linear', C=1.0)\r\n",
    "svm.fit(X_train, y_train)\r\n",
    "\r\n",
    "# Predict on the test set\r\n",
    "y_pred = svm.predict(X_test)\r\n",
    "\r\n",
    "# Print the confusion matrix and classification report\r\n",
    "print(confusion_matrix(y_test, y_pred))\r\n",
    "print(classification_report(y_test, y_pred))\r\n",
    "\r\n",
    "# Visualize the decision boundary\r\n",
    "def plot_svc_decision_boundary(model, ax=None, plot_support=True):\r\n",
    "    if ax is None:\r\n",
    "        ax = plt.gca()\r\n",
    "    xlim = ax.get_xlim()\r\n",
    "    ylim = ax.get_ylim()\r\n",
    "\r\n",
    "    # Create grid to evaluate model\r\n",
    "    xx = np.linspace(xlim[0], xlim[1], 30)\r\n",
    "    yy = np.linspace(ylim[0], ylim[1], 30)\r\n",
    "    YY, XX = np.meshgrid(yy, xx)\r\n",
    "    xy = np.vstack([XX.ravel(), YY.ravel()]).T\r\n",
    "    Z = model.decision_function(xy).reshape(XX.shape)\r\n",
    "\r\n",
    "    # Plot decision boundary and margins\r\n",
    "    ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\r\n",
    "               linestyles=['--', '-', '--'])\r\n",
    "\r\n",
    "    # Plot support vectors\r\n",
    "    if plot_support:\r\n",
    "        ax.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1],\r\n",
    "                   s=100, linewidth=1, facecolors='none', edgecolors='k')\r\n",
    "    ax.set_xlim(xlim)\r\n",
    "    ax.set_ylim(ylim)\r\n",
    "\r\n",
    "# Plot\r\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='autumn')\r\n",
    "plot_svc_decision_boundary(svm)\r\n",
    "plt.title(\"SVM Decision Boundary with Support Vectors\")\r\n",
    "plt.show()\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
