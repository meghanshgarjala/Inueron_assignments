{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f987bc4-2bf6-4296-b7b0-25a8ef7aaeb3",
   "metadata": {},
   "source": [
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
    "\n",
    "\n",
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
    "\n",
    "\n",
    "Q3. How does Bernoulli Naive Bayes handle missing values?\n",
    "\n",
    "\n",
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c319a95c-d2b5-4aa2-8ccd-2881e7353404",
   "metadata": {},
   "source": [
    "1.\n",
    "\n",
    "This problem asks for the conditional probability that an employee is a smoker given that they use the health insurance plan. We can denote:\n",
    "\n",
    "P(H)=0.7 (the probability that an employee uses the health insurance plan)\n",
    "P(S∣H)=0.4 (the probability that an employee is a smoker given they use the health insurance plan)\n",
    "Since the problem directly gives the conditional probability \n",
    "P(S∣H), the answer is:\n",
    "P(Smoker∣Uses Health Insurance)=P(S∣H)=0.4\n",
    "So, the probability is 0.4 or 40%.\n",
    "\n",
    "2.\n",
    "\n",
    "Bernoulli Naive Bayes:\n",
    "\n",
    "Designed for binary or boolean features (e.g., presence/absence of a word in text).\n",
    "Each feature is modeled as following a Bernoulli distribution (0 or 1).\n",
    "It’s particularly suited for binary data where features can be thought of as either present or absent.\n",
    "Multinomial Naive Bayes:\n",
    "\n",
    "Designed for discrete count data (e.g., word counts in text).\n",
    "Each feature is modeled as following a multinomial distribution (e.g., the number of times a word appears).\n",
    "It’s often used in text classification problems where features represent frequencies or counts.\n",
    "Key Difference: Bernoulli Naive Bayes models binary features and is suitable when considering the presence/absence of a feature, while Multinomial Naive Bayes is for discrete count data and is suitable for frequency-based features.\n",
    "\n",
    "3.\n",
    "\n",
    "In Bernoulli Naive Bayes, missing values can be interpreted as the absence of a feature (a feature value of 0). Since the algorithm expects binary features (0 or 1), if a feature is missing, it can simply be treated as a 0. However, this is a simplistic approach, and in practice, it's often better to use imputation techniques to handle missing values or preprocess the data to indicate that the feature was missing.\n",
    "\n",
    "\n",
    "4.\n",
    "\n",
    "Yes, Gaussian Naive Bayes can be used for multi-class classification.\n",
    "\n",
    "Gaussian Naive Bayes assumes that the features are normally distributed and is typically used for continuous data.\n",
    "In the case of multi-class classification, the algorithm calculates the probability of each class given the feature values using Bayes' theorem, and then it selects the class with the highest posterior probability as the predicted class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
