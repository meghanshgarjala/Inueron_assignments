{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b7062b2-f4a9-4beb-8306-83288b546d36",
   "metadata": {},
   "source": [
    "Q1. What is Lasso Regression, and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860c3aaa-d128-4f41-8f32-0144130769b4",
   "metadata": {},
   "source": [
    "Lasso Regression is a type of linear regression that incorporates L1 regularization, also known as the ‚ÄúL1 penalty‚Äù. This penalty adds a term to the cost function equal to the absolute value of the magnitude of coefficients (|Œ≤j|). This encourages simple, sparse models by shrinking some coefficients towards zero, potentially eliminating them entirely. In contrast, Ridge Regression uses L2 regularization, which adds a penalty proportional to the square of the coefficient magnitude (Œ≤j^2). This does not result in coefficient elimination and produces less sparse models.\n",
    "\n",
    "Mathematically, the Lasso Regression cost function can be represented as:\r\n",
    "\r\n",
    "minimize: Œ£(y - XŒ≤)^2 + Œª * Œ£|Œ≤j|\r\n",
    "\r\n",
    "where Œ£ denotes summation, y is the response variable, X is the design matrix, Œ≤ is the coefficient vector, Œª is the regularization parameter, and |Œ≤j| represents the absolute value of the jth coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a041e08d-edf9-4e8c-b108-d901c6f9815d",
   "metadata": {},
   "source": [
    "Q2. What is the main advantage of using Lasso Regression in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5008c9ee-7932-4080-8c4f-56e559d6ebbf",
   "metadata": {},
   "source": [
    "Main Advantage:\n",
    "\n",
    "Sparse Solutions: Lasso regression can shrink some coefficients to exactly zero, thereby automatically performing feature selection. This means that it can identify and retain only the most important features, simplifying the model and potentially improving interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423c22ff-d584-4529-a12c-a37adbd507a5",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the coefficients of a Lasso Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3e4bad-fb0e-47b0-a579-ca84cc132577",
   "metadata": {},
   "source": [
    "Coefficient Interpretation:\n",
    "\n",
    "Magnitude and Sign: The coefficients indicate the direction and strength of the relationship between the predictors and the response variable, similar to other linear regression models.\n",
    "\n",
    "Zero Coefficients: A coefficient of zero means that the corresponding feature has been excluded from the model, indicating it is not important for predicting the response variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fc8673-5c33-486f-9736-f72e45609c1c",
   "metadata": {},
   "source": [
    "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the\r\n",
    "model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7f7400-9a6f-4d42-b23a-c7782b960d18",
   "metadata": {},
   "source": [
    "Tuning Parameters:\n",
    "\n",
    "Lambda (Œª): The main tuning parameter in Lasso regression is the regularization parameter Œª.\n",
    "\n",
    "Effect on Performance:\n",
    "\n",
    "Large Œª: Increases the penalty, leading to more coefficients being shrunk to zero and greater feature selection.\n",
    "\n",
    "Small Œª: Decreases the penalty, resulting in less regularization and more features being included in the model.\n",
    "\n",
    "Alpha (Œ±) in Elastic Net: When using Elastic Net (a combination of Lasso and Ridge), the mixing parameter ùõº can be adjusted to balance between L1 (Lasso) and L2 (Ridge) penalties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4397da34-2909-4f48-8f6e-4382e15601bc",
   "metadata": {},
   "source": [
    "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9918fd25-7eaa-45d4-8fc0-879237964c18",
   "metadata": {},
   "source": [
    "Non-Linear Regression with Lasso:\n",
    "\n",
    "Polynomial Features: By transforming the original features into polynomial features, Lasso regression can be applied to non-linear relationships. This is done by adding polynomial terms (e.g.,ùë•^2,ùë•^3,‚Ä¶) and interaction terms to the model.\n",
    "\n",
    "Generalized Linear Models (GLMs): Lasso can also be applied to GLMs, such as logistic regression for binary classification, to handle non-linear relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dc30a3-5bda-4ab9-9eb4-a350d645442a",
   "metadata": {},
   "source": [
    "Q6. What is the difference between Ridge Regression and Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cd8301-0dfb-42ce-9251-137b08b6a068",
   "metadata": {},
   "source": [
    "Ridge Regression and Lasso Regression are two forms of regularized linear regression, used to address overfitting and improve model interpretability. Both methods introduce penalties to the regression coefficients, but they differ in their approach and suitability for specific problems.\r\n",
    "\r\n",
    "Ridge Regression:\r\n",
    "\r\n",
    "Introduces a penalty term to the regression equation, proportional to the square of the coefficients (L2 regularization).\r\n",
    "Shrinkage occurs uniformly, reducing all coefficients by a similar amount.\r\n",
    "Does not zero out coefficients, meaning all feature variables remain in the \n",
    "model.\r\n",
    "Usefu\n",
    "l when:\r\n",
    "Overfitting is a concern, and a small amount of regularization is needed.\r\n",
    "Feature selection is not essential.\r\n",
    "Model interpretability is \n",
    "important.\r\n",
    "Lasso Regression:\r\n",
    "\r\n",
    "Introduces a penalty term to the regression equation, proportional to the absolute value of the coefficients (L1 regularization).\r\n",
    "Shrinkage occurs disproportionately, reducing some coefficients to zero, effectively eliminating features.\r\n",
    "Can be used for feature selection by automatically selecting the most impo\n",
    "rtant feature\n",
    "s.\r\n",
    "Useful when:\r\n",
    "Feature selection is crucial.\r\n",
    "The number of features is large, and redundancy is present.\r\n",
    "Models with a small number of relevant features are expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095fda76-c2da-4015-89dc-ab93b97f0372",
   "metadata": {},
   "source": [
    "Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75b3b05-b2f4-4ea6-8d2c-8d479d5e470b",
   "metadata": {},
   "source": [
    "Handling Multicollinearity:\n",
    "\n",
    "Yes, Lasso Regression can handle multicollinearity. By shrinking some coefficients to zero, Lasso regression reduces the impact of correlated predictors, effectively selecting one predictor from a group of correlated variables and excluding others. This helps in mitigating the issue of multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c8270d-dd3a-43e3-8dd1-66850cf50aa7",
   "metadata": {},
   "source": [
    "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a01eff-2bfe-4834-aae9-a6f3950490c8",
   "metadata": {},
   "source": [
    "Choosing Optimal Lambda:\n",
    "\n",
    "Cross-Validation: The most common method is k-fold cross-validation to select the optimal Œª. The dataset is split into k subsets, and the model is trained and validated k times, each time using a different subset as the validation set.\n",
    "\n",
    "Grid Search: A range of Œª values is tested, and the value that minimizes the cross-validation error is chosen.\n",
    "\n",
    "Regularization Path: Methods like the LARS (Least Angle Regression) algorithm can compute the solution path for different values of Œª, allowing for efficient selection.\n",
    "\n",
    "Information Criteria: Criteria like AIC (Akaike Information Criterion) or BIC (Bayesian Information Criterion) can also be used to select the optimal ùúÜ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
