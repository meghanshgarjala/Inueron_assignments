{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cb48aa4-dd31-43cd-a151-11646c7b292e",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\r\n",
    "application."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ae05224a-9a1b-4cdb-8ee3-a77ae772d305",
   "metadata": {},
   "source": [
    "The min-max scaling formula transforms a feature value x to a normalized value x' within a specified range, typically [0, 1]. The formula is:\r\n",
    "\r\n",
    "x' = (x - min) / (max - min)\r\n",
    "\r\n",
    "where:\r\n",
    "\r\n",
    "x is the oril fe\n",
    " value\r\n",
    "min is the minimum valuhe ,\n",
    ",\n",
    "feature\r\n",
    "max is the maxiemoum va the,\n",
    ",\n",
    "\n",
    " feature\r\n",
    "This formula subtracts the minimum value from x, then divides the result by the range (max - min) to scale the value to the desiredExample:\n",
    "\n",
    "Suppose you have a dataset with the feature values [10, 20, 30, 40, 50], and you want to scale them to the range [10,20,30,40,50]‚Üí[0,0.25,0.5,0.75,1]160) / 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810fe778-ee29-4a3d-b0ea-b4f95cc2945e",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\r\n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2b70cb-78d5-4aa7-8187-47a168a0941a",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as normalization to unit length, scales a feature vector so that its Euclidean length (L2 norm) is 1. This technique is particularly useful when you want to compare vectors on the same scale.\n",
    "\n",
    "Difference from Min-Max Scaling:\n",
    "\n",
    "Min-Max scaling transforms each feature individually to a specific range.\n",
    "\n",
    "Unit Vector scaling normalizes the entire feature vector to unit length, keeping the relative magnitudes intact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09699226-dc52-4a3e-b687-254c6e88035f",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\r\n",
    "example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9701e197-f72e-4f94-8ce0-aabd31db5dcc",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is a statistical technique used for dimensionality reduction by transforming the data into a new coordinate system where the greatest variances by any projection of the data come to lie on the first few principal components.\n",
    "\n",
    "Steps:\n",
    "\n",
    "Standardize the data.\n",
    "Compute the covariance matrix.\n",
    "Calculate the eigenvalues and eigenvectors of the covariance matrix.\n",
    "Sort the eigenvalues and select the top \n",
    "ùëò\n",
    "k eigenvectors (principal components).\n",
    "Transform the data onto the new subspace.\n",
    "Example:\n",
    "Suppose you have a dataset with features X1 and X2:\n",
    "‚Äã\n",
    "[2.5 2.4]\n",
    "[0.5 0.7]\n",
    "[2.2 2.9]\n",
    "[1.9 2.2]\n",
    "[3.1 3.0]\n",
    "\n",
    "1.Standardize the data:\n",
    "\n",
    "[0.69 0.49\n",
    "‚àí1.31 ‚àí1.21\n",
    "0.39 0.99\n",
    "0.09 0.29\n",
    "1.29 1.09]\n",
    "‚Äã\n",
    "\n",
    "2.Compute the covariance matrix, find eigenvalues and eigenvectors, and select the top principal components to transform the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8130b5-8ea1-4f1e-a757-fb5cd1a88dca",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\r\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6cd625-b02f-4cb6-9d68-0cde75916cbe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d1d0cb7-afa0-4458-a0bd-75554853e4bb",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\r\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\r\n",
    "preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35239f71-c39e-46c9-a2c2-b56e4556b34b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ec71771-e972-431f-9be3-68211b8659db",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\r\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\r\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32566b0-d542-41c8-bd1f-8622d66ae99c",
   "metadata": {},
   "source": [
    "To use PCA for dimensionality reduction:\n",
    "\n",
    "Standardize the data: Scale all features to have zero mean and unit variance.\n",
    "\n",
    "Compute the covariance matrix: Measure how features vary together.\n",
    "\n",
    "Calculate eigenvalues and eigenvectors: Identify the principal components.\n",
    "\n",
    "Select top components: Choose a subset of principal components that explain a large portion of the variance (e.g., 95%).\n",
    "Transform the data: Project the original data onto the selected components.\n",
    "\n",
    "Example:\n",
    "\n",
    "If you have 50 features, you might find that 10 principal components explain 95% of the variance. Transform the original dataset using these 10 components, reducing the dimensionality from 50 to 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d81709-1483-4783-addc-e09a7ee6600c",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\r\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7b6a0f-3f2f-4c55-9fb0-aed76cd61035",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb1a5338-3c87-4010-bd8c-67068dd40fff",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\r\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7835284-d2b8-4053-a64f-36e05aef6e60",
   "metadata": {},
   "source": [
    "To determine the number of principal components to retain:\n",
    "\n",
    "Standardize the features: Ensure all features have zero mean and unit variance.\n",
    "\n",
    "Compute the covariance matrix.\n",
    "\n",
    "Calculate eigenvalues and eigenvectors.\n",
    "\n",
    "Explain variance: Calculate the cumulative explained variance for each principal component.\n",
    "\n",
    "Suppose after performing PCA, you get the following explained variance for the components:\n",
    "\n",
    "PC1: 50%\n",
    "\n",
    "PC2: 20%\n",
    "\n",
    "PC3: 15%\n",
    "\n",
    "PC4: 10%\n",
    "\n",
    "PC5: 5%\n",
    "\n",
    "To retain 95% of the variance, you would select the first four principal components:\n",
    "\n",
    "50%+20%+15%+10%=95%\n",
    "\n",
    "Thus, you would retain 4 principal components because they explain most of the variance in the dataset, reducing dimensionality while preserving the majority of the information.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209925d7-1aff-4b5d-8bd9-da967e10846d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03996294-bbf8-4c30-b23e-64ad2ae9afe8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8aab29b0-7801-4f68-8cca-aaf26a9935ad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e3e4a80-7b64-44b6-bfa7-cca8563bcfb3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a771cc3-2ef4-4862-8667-43eb7c35ea2d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55e8b322-4993-404a-abed-8327f29e6b66",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f52b149-4570-4f7d-9da0-ed666220e0ae",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c379f89b-b784-46d1-a612-a637cb5c966f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
