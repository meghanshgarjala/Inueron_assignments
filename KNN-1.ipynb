{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f5691d6-dc9b-418e-b2b8-1e01b1527ddf",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?\n",
    "\n",
    "\n",
    "Q2. How do you choose the value of K in KNN?\n",
    "\n",
    "\n",
    "Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "\n",
    "\n",
    "Q4. How do you measure the performance of KNN?\n",
    "\n",
    "\n",
    "Q5. What is the curse of dimensionality in KNN?\n",
    "\n",
    "\n",
    "Q6. How do you handle missing values in KNN?\n",
    "\n",
    "\n",
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n",
    "\n",
    "\n",
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n",
    "\n",
    "\n",
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "\n",
    "\n",
    "Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f4767b-679d-457a-938e-755082c00444",
   "metadata": {},
   "source": [
    "Q1. What is the KNN Algorithm?\n",
    "The K-Nearest Neighbors (KNN) algorithm is a simple, non-parametric, instance-based learning method used for classification and regression tasks. It classifies or predicts the target value of a new data point based on the K closest training examples in the feature space. In classification, it assigns the class that is most common among the nearest neighbors. In regression, it predicts the average of the target values of the nearest neighbors.\n",
    "\n",
    "\n",
    "Q2. \n",
    "\n",
    "\n",
    "Choosing the value of K is crucial as it significantly affects the model‚Äôs performance:\n",
    "\n",
    "Small K (e.g., K = 1 or 2): The model may capture noise in the data, leading to overfitting.\n",
    "Large K: The model might become too generalized, underfitting the data and missing important patterns.\n",
    "Common approaches to choose K:\n",
    "\n",
    "Cross-Validation: Use cross-validation to evaluate the performance of different K values and select the one that minimizes error.\n",
    "Square Root Heuristic: A rule of thumb is to choose K as the square root of the total number of training samples, \n",
    "K= N.\n",
    "Odd K Values: For classification, using odd K values helps avoid ties between classes.\n",
    "\n",
    "\n",
    "Q3\n",
    "\n",
    "\n",
    "KNN Classifier: Used for classification tasks; it assigns a class to a data point based on the majority class among its K nearest neighbors.\n",
    "KNN Regressor: Used for regression tasks; it predicts a continuous value by averaging the target values of the K nearest neighbors.\n",
    "\n",
    "\n",
    "Q4.\n",
    "\n",
    "\n",
    "Classification Metrics:\n",
    "Accuracy: Proportion of correctly classified instances.\n",
    "Confusion Matrix: To measure precision, recall, F1-score, etc.\n",
    "ROC-AUC: Measures the performance of classification models at different thresholds.\n",
    "Regression Metrics:\n",
    "Mean Squared Error (MSE): Average squared difference between predicted and actual values.\n",
    "Mean Absolute Error (MAE): Average absolute difference between predicted and actual values.\n",
    "R-squared (R¬≤): Proportion of variance in the dependent variable explained by the model.\n",
    "\n",
    "\n",
    "Q5. \n",
    "\n",
    "\n",
    "The curse of dimensionality refers to the problem that arises when data has too many features (high-dimensional data). In high dimensions, the concept of distance becomes less meaningful because the nearest and farthest neighbors are almost equidistant. This dilutes the KNN's ability to find truly \"nearest\" neighbors, leading to degraded performance.\n",
    "\n",
    "\n",
    "Q6. \n",
    "\n",
    "\n",
    "Imputation: Impute missing values using mean, median, or mode for continuous features and the most frequent class for categorical features.\n",
    "KNN Imputation: Use KNN to impute missing values by averaging the values of the nearest neighbors (neighbors without missing values).\n",
    "Remove Rows or Columns: If missing values are minimal, consider removing the affected rows or columns.\n",
    "\n",
    "\n",
    "Q7.\n",
    "\n",
    "\n",
    "KNN Classifier: Best for classification problems where class boundaries are well-defined and the data is well separated. It is particularly useful for multi-class classification tasks.\n",
    "KNN Regressor: Works well for regression problems where the relationship between the target and features is not linear. However, it can be sensitive to outliers, as they can skew the average.\n",
    "Performance Contrast:\n",
    "\n",
    "KNN Classifier: Performs well with discrete outputs and less sensitive to outliers when compared to regression.\n",
    "KNN Regressor: Can struggle in high-dimensional or noisy data but works well for predicting continuous values.\n",
    "\n",
    "\n",
    "Q8. What are the Strengths and Weaknesses of the KNN Algorithm for Classification and Regression Tasks, and How Can These Be Addressed?\n",
    "Strengths:\n",
    "\n",
    "Simple and Intuitive: Easy to understand and implement.\n",
    "Non-Parametric: Makes no assumptions about the underlying data distribution.\n",
    "Adaptable: Can be used for both classification and regression.\n",
    "Weaknesses:\n",
    "\n",
    "Computationally Expensive: Slow with large datasets due to the need to compute distances for each prediction.\n",
    "Sensitive to Noisy Data: Outliers can significantly affect predictions.\n",
    "Feature Scaling Needed: Performance depends on the scale of data; requires normalization or standardization.\n",
    "Addressing Weaknesses:\n",
    "\n",
    "Dimensionality Reduction: Use techniques like PCA to reduce features.\n",
    "Feature Scaling: Standardize or normalize data to improve distance calculations.\n",
    "Efficient Data Structures: Use KD-trees or Ball trees to speed up neighbor searches.\n",
    "Q9. What is the Difference Between Euclidean Distance and Manhattan Distance in KNN?\n",
    "Euclidean Distance: Measures the straight-line distance between two points in space. Formula: \n",
    "‚àë\n",
    "(\n",
    "ùë•\n",
    "ùëñ\n",
    "‚àí\n",
    "ùë¶\n",
    "ùëñ\n",
    ")\n",
    "2\n",
    "‚àë(x \n",
    "i\n",
    "‚Äã\n",
    " ‚àíy \n",
    "i\n",
    "‚Äã\n",
    " ) \n",
    "2\n",
    " \n",
    "‚Äã\n",
    " . Best suited when data features have similar scales and continuous relationships.\n",
    "\n",
    "Manhattan Distance: Measures the distance between two points by summing the absolute differences of their coordinates. Formula: \n",
    "‚àë\n",
    "‚à£\n",
    "ùë•\n",
    "ùëñ\n",
    "‚àí\n",
    "ùë¶\n",
    "ùëñ\n",
    "‚à£\n",
    "‚àë‚à£x \n",
    "i\n",
    "‚Äã\n",
    " ‚àíy \n",
    "i\n",
    "‚Äã\n",
    " ‚à£. Preferred when dealing with high-dimensional data or when features have varying scales.\n",
    "\n",
    "Comparison:\n",
    "\n",
    "Euclidean Distance captures the direct path and is sensitive to large variations in feature scales.\n",
    "Manhattan Distance follows a grid-like path and is less sensitive to scale changes, making it suitable for cases with categorical or integer data.\n",
    "Q10. What is the Role of Feature Scaling in KNN?\n",
    "Feature scaling is critical in KNN because the algorithm relies on distance metrics (e.g., Euclidean, Manhattan). If features have different scales (e.g., age in years vs. income in thousands), the feature with a larger range will disproportionately influence the distance calculation. To prevent this:\n",
    "\n",
    "Normalization: Scales the data between 0 and 1.\n",
    "Standardization: Centers the data around the mean with a standard deviation of 1.\n",
    "Scaling ensures that all features contribute equally to the distance calculations, leading to more reliable and meaningful predictions from the KNN algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
