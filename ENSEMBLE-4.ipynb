{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fe9eecc-e4ca-4cb2-832b-1b1aa6804d49",
   "metadata": {},
   "source": [
    "Build a random forest classifier to predict the risk of heart disease based on a dataset of patient \n",
    "information. The dataset contains 303 instances with 14 features, including age, sex, chest pain type\r\n",
    "resting blood pressure, serum cholesterol, and maximum heart rate achieved.\r\n",
    "Dataset link: https://drive.google.com/file/d/1bGoIE4Z2kG5nyh-fGZAJ7LH0ki3UfmSJ/view?\r\n",
    "usp=share_\n",
    "\n",
    "link\r\n",
    "Q1. Preprocess the dataset by handling missing values, encoding categorical variables, and scaling the\r\n",
    "numerical features if nece\n",
    "\n",
    "ssary.\r\n",
    "Q2. Split the dataset into a training set (70%) and a test set\n",
    "\n",
    " (30%).\r\n",
    "Q3. Train a random forest classifier on the training set using 100 trees and a maximum depth of 10 for each\r\n",
    "tree. Use the default values for other hyperpa\n",
    "\n",
    "rameters.\r\n",
    "Q4. Evaluate the performance of the model on the test set using accuracy, precision, recall, and\n",
    "\n",
    " F1 score.\r\n",
    "Q5. Use the feature importance scores to identify the top 5 most important features in predicting heart\r\n",
    "disease risk. Visualise the feature importances using \n",
    "\n",
    "a bar chart.\r\n",
    "Q6. Tune the hyperparameters of the random forest classifier using grid search or rando  search. Try\r\n",
    "different values of the number of trees, maximum depth, minimum samples split, and m nimum samples\r\n",
    "leaf. Use 5-fold cross-validation to evaluate the performance of each set of h\n",
    "\n",
    "yperparameters.\r\n",
    "Q7. Report the best set of hyperparameters found by the search and the correspon ing performance\r\n",
    "metrics. Compare the performance of the tuned model with t\n",
    "\n",
    "he default model.\r\n",
    "Q8. Interpret the model by analysing the decision boundaries of the random forest clssifier. Plot the\r\n",
    "decision boundaries on a scatter plot of two of the most important features. Discuss the insights and\r\n",
    "limitations of the model for predicting heart disease risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62242302-8c77-447c-a188-a565749cc81e",
   "metadata": {},
   "source": [
    "Step 1: Preprocess the Dataset\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('heart.csv')\n",
    "\n",
    "# Handle missing values\n",
    "# Assuming no missing values, if there are, we can use SimpleImputer for filling\n",
    "\n",
    "# Encode categorical variables\n",
    "categorical_features = ['sex', 'cp', 'restecg', 'slope', 'thal', 'ca']\n",
    "numeric_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "\n",
    "# Preprocessing pipelines\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Preprocess the data\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "X = preprocessor.fit_transform(X)\n",
    "\n",
    "\n",
    "\n",
    "Step 2: Split the Dataset\n",
    "\n",
    "\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "Step 3: Train a Random Forest Classifier\n",
    "\n",
    "\n",
    "# Train the Random Forest Classifier\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "Step 4: Evaluate Model Performance\n",
    "\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "\n",
    "\n",
    "Step 5: Feature Importance\n",
    "\n",
    "\n",
    "# Feature Importance\n",
    "importances = rf.feature_importances_\n",
    "indices = np.argsort(importances)[-5:]\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.barh(range(len(indices)), importances[indices], align=\"center\")\n",
    "plt.yticks(range(len(indices)), [df.columns[i] for i in indices])\n",
    "plt.xlabel(\"Relative Importance\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "Step 6: Hyperparameter Tuning\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "\n",
    "Step 7: Report Best Hyperparameters\n",
    "\n",
    "\n",
    "# Evaluate the best model\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred_best = best_rf.predict(X_test)\n",
    "\n",
    "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
    "precision_best = precision_score(y_test, y_pred_best)\n",
    "recall_best = recall_score(y_test, y_pred_best)\n",
    "f1_best = f1_score(y_test, y_pred_best)\n",
    "\n",
    "print(f\"Best Model Accuracy: {accuracy_best}\")\n",
    "print(f\"Best Model Precision: {precision_best}\")\n",
    "print(f\"Best Model Recall: {recall_best}\")\n",
    "print(f\"Best Model F1 Score: {f1_best}\")\n",
    "\n",
    "# Compare with the default model\n",
    "print(f\"Default Model vs Best Model - Accuracy: {accuracy} vs {accuracy_best}\")\n",
    "print(f\"Default Model vs Best Model - Precision: {precision} vs {precision_best}\")\n",
    "print(f\"Default Model vs Best Model - Recall: {recall} vs {recall_best}\")\n",
    "print(f\"Default Model vs Best Model - F1 Score: {f1} vs {f1_best}\")\n",
    "\n",
    "\n",
    "Step 8: Interpret the Model\n",
    "\n",
    "\n",
    "# Plot decision boundaries (using two most important features)\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_decision_boundaries(clf, X, y):\n",
    "    # Create a meshgrid for plotting\n",
    "    h = .02  # step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # Predict on meshgrid\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Plot contour and training examples\n",
    "    plt.contourf(xx, yy, Z, alpha=0.8, cmap=ListedColormap(('orange', 'blue')))\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o', s=50, cmap=ListedColormap(('orange', 'blue')))\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.title(\"Decision Boundary\")\n",
    "    plt.show()\n",
    "\n",
    "# For simplicity, let's use the first two most important features\n",
    "X_two_features = X[:, indices[-2:]]\n",
    "plot_decision_boundaries(best_rf, X_two_features, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
