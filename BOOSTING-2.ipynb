{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "295d1dbb-1e86-461e-91b5-1427718a3864",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?\n",
    "\n",
    "\n",
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared.\n",
    "\n",
    "\n",
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters.\n",
    "\n",
    "\n",
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "\n",
    "\n",
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "\n",
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "\n",
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1885c2-c990-464c-af6e-5f28ed3433cc",
   "metadata": {},
   "source": [
    "Q1.\n",
    "\n",
    "\n",
    "Gradient Boosting Regression is a powerful ensemble learning technique used for regression tasks. It builds a predictive model by sequentially adding weak learners, typically decision trees, in a stage-wise fashion. Each new tree is trained to minimize the residual errors (differences between the actual and predicted values) of the current model using gradient descent. The key idea is to combine multiple weak models to create a strong predictive model that optimizes a loss function, such as mean squared error.\n",
    "\n",
    "\n",
    "Q2. \n",
    "\n",
    "\n",
    "Below is a simple implementation of a Gradient Boosting Regressor from scratch using Python and NumPy. This example uses a synthetic dataset and evaluates the model using Mean Squared Error (MSE) and R-squared metrics.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Generate a simple dataset\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 2 * X.flatten() + np.random.normal(0, 1, 100)  # Linear relationship with noise\n",
    "\n",
    "# Split into training and test sets\n",
    "split_index = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        self.initial_prediction = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initial prediction is the mean of y\n",
    "        self.initial_prediction = np.mean(y)\n",
    "        predictions = np.full(y.shape, self.initial_prediction)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Calculate residuals\n",
    "            residuals = y - predictions\n",
    "            \n",
    "            # Fit a decision tree to the residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "            self.trees.append(tree)\n",
    "            \n",
    "            # Update predictions\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.full(X.shape[0], self.initial_prediction)\n",
    "        for tree in self.trees:\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "        return predictions\n",
    "\n",
    "# Simple Decision Tree Regressor implementation\n",
    "class DecisionTreeRegressor:\n",
    "    def __init__(self, max_depth=3):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y, depth=0)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_single(x, self.tree) for x in X])\n",
    "    \n",
    "    def _build_tree(self, X, y, depth):\n",
    "        if depth == self.max_depth or len(set(y)) == 1:\n",
    "            return np.mean(y)\n",
    "        \n",
    "        best_split = None\n",
    "        best_mse = float('inf')\n",
    "        best_left, best_right = None, None\n",
    "        \n",
    "        for feature in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                left_mask = X[:, feature] <= threshold\n",
    "                right_mask = ~left_mask\n",
    "                \n",
    "                if len(y[left_mask]) == 0 or len(y[right_mask]) == 0:\n",
    "                    continue\n",
    "                \n",
    "                left_mse = np.var(y[left_mask]) * len(y[left_mask])\n",
    "                right_mse = np.var(y[right_mask]) * len(y[right_mask])\n",
    "                mse = (left_mse + right_mse) / len(y)\n",
    "                \n",
    "                if mse < best_mse:\n",
    "                    best_split = (feature, threshold)\n",
    "                    best_mse = mse\n",
    "                    best_left, best_right = (X[left_mask], y[left_mask]), (X[right_mask], y[right_mask])\n",
    "        \n",
    "        if best_split is None:\n",
    "            return np.mean(y)\n",
    "        \n",
    "        left_tree = self._build_tree(*best_left, depth + 1)\n",
    "        right_tree = self._build_tree(*best_right, depth + 1)\n",
    "        return (best_split, left_tree, right_tree)\n",
    "    \n",
    "    def _predict_single(self, x, node):\n",
    "        if not isinstance(node, tuple):\n",
    "            return node\n",
    "        \n",
    "        feature, threshold = node[0]\n",
    "        left_tree, right_tree = node[1], node[2]\n",
    "        if x[feature] <= threshold:\n",
    "            return self._predict_single(x, left_tree)\n",
    "        else:\n",
    "            return self._predict_single(x, right_tree)\n",
    "\n",
    "# Train the Gradient Boosting model\n",
    "model = GradientBoostingRegressor(n_estimators=50, learning_rate=0.1, max_depth=2)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R-squared: {r2:.4f}\")\n",
    "\n",
    "\n",
    "Q3.\n",
    "\n",
    "\n",
    "To optimize the performance of the Gradient Boosting model, you can use techniques like Grid Search or Random Search to find the best combination of hyperparameters (e.g., n_estimators, learning_rate, and max_depth). Here's a brief outline:\n",
    "\n",
    "Define the parameter grid:\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [1, 2, 3, 4]\n",
    "}\n",
    "Use Grid Search or Random Search to find the optimal parameters.\n",
    "Evaluate using Mean Squared Error and R-squared to select the best model.\n",
    "\n",
    "\n",
    "Q4\n",
    "\n",
    "\n",
    "A weak learner in Gradient Boosting is a simple model that performs slightly better than random guessing. It typically has limited predictive power (e.g., shallow decision trees with a small depth) but is combined sequentially with other weak learners to improve performance and create a strong model.\n",
    "\n",
    "\n",
    "Q5. \n",
    "\n",
    "\n",
    "The intuition behind Gradient Boosting is to build a strong model by iteratively adding models that correct the errors of the previous ones. Each new model is trained to predict the residuals (errors) of the current combined model, gradually improving the overall predictions by minimizing the loss function step by step.\n",
    "\n",
    "\n",
    "Q6.\n",
    "\n",
    "\n",
    "Gradient Boosting builds an ensemble by:\n",
    "\n",
    "Starting with an initial model (e.g., a constant prediction).\n",
    "Sequentially adding weak learners (e.g., decision trees) that predict the residual errors of the current ensemble.\n",
    "Updating the ensemble model by adding the predictions of the weak learners, scaled by a learning rate.\n",
    "Repeating this process until the desired number of learners is reached or performance plateaus.\n",
    "\n",
    "\n",
    "Q7. \n",
    "\n",
    "\n",
    "Initialize the Model: Start with an initial prediction, often the mean of the target variable.\n",
    "Calculate Residuals: Compute the residuals (errors) between actual and predicted values.\n",
    "Fit a Weak Learner: Train a weak learner on the residuals.\n",
    "Update Model: Adjust the model by adding the predictions of the weak learner, scaled by the learning rate.\n",
    "Repeat: Iterate steps 2-4, adding more learners to minimize the residuals further.\n",
    "Minimize Loss Function: Use gradient descent to optimize the loss function by adjusting the parameters of each weak learner."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
